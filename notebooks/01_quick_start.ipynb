{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§ - Model Testi\n",
                "\n",
                "**ðŸ†“ Colab T4 Native modeller:** Qwen2-VL-2B, Phi-3, InternVL2-2B\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Path ayarÄ±\n",
                "import sys, os\n",
                "PROJECT_ROOT = os.path.dirname(os.getcwd()) if os.getcwd().endswith('notebooks') else os.getcwd()\n",
                "if os.path.exists(os.path.join(PROJECT_ROOT, 'src')):\n",
                "    sys.path.insert(0, PROJECT_ROOT)\n",
                "else:\n",
                "    PROJECT_ROOT = os.getcwd()\n",
                "    sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "import torch\n",
                "from PIL import Image\n",
                "print(f\"Project: {PROJECT_ROOT}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.utils import PromptManager, print_model_table, FREE_MODELS, COLAB_MODELS\n",
                "print_model_table()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pm = PromptManager()\n",
                "print(\"Promptlar:\", pm.list_prompts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Qwen2-VL-2B Testi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
                "\n",
                "MODEL = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
                "print(f\"ðŸ”„ {MODEL} yÃ¼kleniyor...\")\n",
                "processor = AutoProcessor.from_pretrained(MODEL)\n",
                "model = Qwen2VLForConditionalGeneration.from_pretrained(MODEL, torch_dtype=torch.float16, device_map=\"auto\")\n",
                "print(\"âœ… YÃ¼klendi!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test gÃ¶rÃ¼ntÃ¼sÃ¼\n",
                "test_path = os.path.join(PROJECT_ROOT, \"test_xray.png\")\n",
                "if not os.path.exists(test_path):\n",
                "    import urllib.request\n",
                "    urllib.request.urlretrieve(\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Chest_Xray_PA_3-8-2010.png/440px-Chest_Xray_PA_3-8-2010.png\", test_path)\n",
                "\n",
                "image = Image.open(test_path).convert(\"RGB\")\n",
                "prompt = pm.get_prompt(\"rrg\", \"detailed\").user_prompt\n",
                "print(f\"ðŸ“ Prompt: {len(prompt)} chars\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": prompt}]}]\n",
                "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
                "inputs = processor(text=[text], images=[image], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
                "output_ids = model.generate(**inputs, max_new_tokens=512)\n",
                "output = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
                "\n",
                "print(\"\\nðŸ“‹ RAPOR:\")\n",
                "print(\"=\"*50)\n",
                "print(output)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prompt KarÅŸÄ±laÅŸtÄ±rmasÄ±"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = {}\n",
                "for name in [\"baseline\", \"detailed\", \"turkish\"]:\n",
                "    print(f\"\\nðŸ”„ {name}...\")\n",
                "    p = pm.get_prompt(\"rrg\", name).user_prompt\n",
                "    msgs = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": p}]}]\n",
                "    txt = processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
                "    inp = processor(text=[txt], images=[image], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
                "    out = model.generate(**inp, max_new_tokens=512)\n",
                "    results[name] = processor.batch_decode(out, skip_special_tokens=True)[0]\n",
                "    print(f\"âœ… {len(results[name])} chars\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for n, r in results.items():\n",
                "    print(f\"\\n{'='*50}\\nðŸ“‹ {n.upper()}\\n{'='*50}\")\n",
                "    print(r[:300] + \"...\" if len(r) > 300 else r)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Temizlik\n",
                "del model, processor\n",
                "import gc; gc.collect(); torch.cuda.empty_cache()\n",
                "print(f\"ðŸ§¹ GPU: {torch.cuda.memory_allocated()/1e9:.1f} GB\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}