{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ¥ Radyoloji LLM KarÅŸÄ±laÅŸtÄ±rma\n",
                "\n",
                "**ðŸ†“ TÃ¼m modeller Ã¼cretsiz!**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1ï¸âƒ£ Kurulum"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Projeyi GitHub'dan indir\n",
                "!git clone https://github.com/Hanketsu3/LLMComparison.git\n",
                "import os\n",
                "os.chdir('/content/LLMComparison')\n",
                "print(f\"ðŸ“ Dizin: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys, gc\n",
                "from PIL import Image\n",
                "\n",
                "PROJECT_ROOT = '/content/LLMComparison'\n",
                "sys.path.insert(0, PROJECT_ROOT)\n",
                "print(f\"âœ… PROJECT_ROOT: {PROJECT_ROOT}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available(): print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install -q transformers>=4.40.0 accelerate bitsandbytes\n",
                "%pip install -q Pillow pyyaml tqdm pandas scikit-learn evaluate rouge-score"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2ï¸âƒ£ Model Registry"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.utils import PromptManager, print_model_table\n",
                "print_model_table()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pm = PromptManager()\n",
                "print(\"Promptlar:\", pm.list_prompts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3ï¸âƒ£ Test GÃ¶rÃ¼ntÃ¼sÃ¼"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_path = f\"{PROJECT_ROOT}/src/img/1280px-Normal_posteroanterior_(PA)_chest_radiograph_(X-ray).jpg\"\n",
                "test_image = Image.open(test_path).convert(\"RGB\")\n",
                "print(f\"ðŸ“· {test_image.size}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4ï¸âƒ£ Qwen2-VL-2B Testi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
                "\n",
                "MODEL = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
                "print(f\"ðŸ”„ {MODEL}...\")\n",
                "processor = AutoProcessor.from_pretrained(MODEL)\n",
                "model = Qwen2VLForConditionalGeneration.from_pretrained(MODEL, torch_dtype=torch.float16, device_map=\"auto\")\n",
                "print(\"âœ… YÃ¼klendi!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_report(image, prompt_text):\n",
                "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": prompt_text}]}]\n",
                "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
                "    inputs = processor(text=[text], images=[image], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
                "    output_ids = model.generate(**inputs, max_new_tokens=512)\n",
                "    return processor.batch_decode(output_ids, skip_special_tokens=True)[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "prompt = pm.get_prompt(\"rrg\", \"detailed\").user_prompt\n",
                "report = generate_report(test_image, prompt)\n",
                "print(\"ðŸ“‹ RAPOR:\\n\" + \"=\"*50 + \"\\n\" + report)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5ï¸âƒ£ Prompt KarÅŸÄ±laÅŸtÄ±rmasÄ±"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt_results = {}\n",
                "for name in [\"baseline\", \"detailed\", \"turkish\"]:\n",
                "    print(f\"ðŸ”„ {name}...\")\n",
                "    prompt_results[name] = generate_report(test_image, pm.get_prompt(\"rrg\", name).user_prompt)\n",
                "    print(f\"âœ… {len(prompt_results[name])} chars\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for n, r in prompt_results.items():\n",
                "    print(f\"\\n{'='*50}\\nðŸ“‹ {n.upper()}\\n{'='*50}\\n{r[:400]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "del model, processor; gc.collect(); torch.cuda.empty_cache()\n",
                "print(f\"ðŸ§¹ GPU: {torch.cuda.memory_allocated()/1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6ï¸âƒ£ Kaydet & GitHub'a Push"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "from datetime import datetime\n",
                "\n",
                "os.makedirs(f\"{PROJECT_ROOT}/results\", exist_ok=True)\n",
                "output_path = f\"{PROJECT_ROOT}/results/experiment_{datetime.now().strftime('%Y%m%d_%H%M')}.json\"\n",
                "\n",
                "with open(output_path, \"w\") as f:\n",
                "    json.dump({\"timestamp\": datetime.now().isoformat(), \"prompt_comparison\": prompt_results}, f, indent=2, ensure_ascii=False)\n",
                "print(f\"âœ… Kaydedildi: {output_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GitHub'a push - sonuÃ§larÄ± bilgisayarÄ±na almak iÃ§in\n",
                "# âš ï¸ Token'Ä±nÄ± gir (GitHub Settings > Developer settings > Personal access tokens)\n",
                "GITHUB_TOKEN = \"ghp_XXXXX\"  # Token'Ä±nÄ± buraya yapÄ±ÅŸtÄ±r\n",
                "\n",
                "!git config user.email \"you@example.com\"\n",
                "!git config user.name \"Your Name\"\n",
                "!git add results/\n",
                "!git commit -m \"Add experiment results\"\n",
                "!git push https://{GITHUB_TOKEN}@github.com/Hanketsu3/LLMComparison.git main\n",
                "\n",
                "print(\"\\nâœ… SonuÃ§lar GitHub'a push edildi!\")\n",
                "print(\"ðŸ’» BilgisayarÄ±nda: cd LLMComparison && git pull\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}