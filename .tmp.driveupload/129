{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üè• Radyoloji LLM Kar≈üƒ±la≈ütƒ±rma\n",
                "\n",
                "**üÜì T√ºm modeller √ºcretsiz!**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Kurulum"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Projeyi GitHub'dan indir\n",
                "!git clone https://github.com/Hanketsu3/LLMComparison.git\n",
                "%cd LLMComparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys, os, gc\n",
                "from PIL import Image\n",
                "\n",
                "PROJECT_ROOT = \"/content/LLMComparison\"\n",
                "sys.path.insert(0, PROJECT_ROOT)\n",
                "print(f\"‚úÖ PROJECT_ROOT: {PROJECT_ROOT}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available(): print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install -q transformers>=4.40.0 accelerate bitsandbytes\n",
                "%pip install -q Pillow pyyaml tqdm pandas scikit-learn evaluate rouge-score"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Model Registry"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.utils import PromptManager, print_model_table\n",
                "print_model_table()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pm = PromptManager()\n",
                "print(\"Promptlar:\", pm.list_prompts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Test G√∂r√ºnt√ºs√º"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Projede bulunan test g√∂r√ºnt√ºs√º\n",
                "test_path = os.path.join(PROJECT_ROOT, \"src/img/1280px-Normal_posteroanterior_(PA)_chest_radiograph_(X-ray).jpg\")\n",
                "test_image = Image.open(test_path).convert(\"RGB\")\n",
                "print(f\"üì∑ {test_image.size}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Qwen2-VL-2B Testi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
                "\n",
                "MODEL = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
                "print(f\"üîÑ {MODEL}...\")\n",
                "processor = AutoProcessor.from_pretrained(MODEL)\n",
                "model = Qwen2VLForConditionalGeneration.from_pretrained(MODEL, torch_dtype=torch.float16, device_map=\"auto\")\n",
                "print(\"‚úÖ Y√ºklendi!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_report(image, prompt_text):\n",
                "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": prompt_text}]}]\n",
                "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
                "    inputs = processor(text=[text], images=[image], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
                "    output_ids = model.generate(**inputs, max_new_tokens=512)\n",
                "    return processor.batch_decode(output_ids, skip_special_tokens=True)[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "prompt = pm.get_prompt(\"rrg\", \"detailed\").user_prompt\n",
                "report = generate_report(test_image, prompt)\n",
                "print(\"üìã RAPOR:\\n\" + \"=\"*50 + \"\\n\" + report)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Prompt Kar≈üƒ±la≈ütƒ±rmasƒ±"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt_results = {}\n",
                "for name in [\"baseline\", \"detailed\", \"turkish\"]:\n",
                "    print(f\"üîÑ {name}...\")\n",
                "    prompt_results[name] = generate_report(test_image, pm.get_prompt(\"rrg\", name).user_prompt)\n",
                "    print(f\"‚úÖ {len(prompt_results[name])} chars\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for n, r in prompt_results.items():\n",
                "    print(f\"\\n{'='*50}\\nüìã {n.upper()}\\n{'='*50}\\n{r[:400]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "del model, processor; gc.collect(); torch.cuda.empty_cache()\n",
                "print(f\"üßπ GPU: {torch.cuda.memory_allocated()/1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Kaydet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "from datetime import datetime\n",
                "\n",
                "results_dir = os.path.join(PROJECT_ROOT, \"results\")\n",
                "os.makedirs(results_dir, exist_ok=True)\n",
                "\n",
                "output_path = os.path.join(results_dir, f\"experiment_{datetime.now().strftime('%Y%m%d_%H%M')}.json\")\n",
                "with open(output_path, \"w\") as f:\n",
                "    json.dump({\"timestamp\": datetime.now().isoformat(), \"prompt_comparison\": prompt_results}, f, indent=2, ensure_ascii=False)\n",
                "print(f\"‚úÖ Kaydedildi: {output_path}\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}